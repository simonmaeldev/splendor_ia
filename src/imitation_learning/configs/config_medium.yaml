# Medium Architecture Configuration (Default)
# Recommended starting point for training

seed: 42

data:
  data_root: "data/games"
  processed_dir: "data/processed"
  train_ratio: 0.8
  val_ratio: 0.1
  test_ratio: 0.1

preprocessing:
  num_workers: null  # null = auto-detect (cpu_count - 2), or specify number
  parallel: true     # Use parallel/optimized processing
  verbose: true      # Detailed logging
  batch_size: 500    # Process and save files in batches (reduces memory)
  use_float32: true  # Use float32 instead of float64 (50% memory savings)
  monitor_memory: true  # Enable memory monitoring during processing
  intermediate_dir: "data/intermediate"  # Directory for batch files
  merge_strategy: "sequential"  # "sequential" (memory-efficient) or "accumulative" (legacy)

model:
  input_dim: null
  trunk_dims: [512, 256]  # Standard trunk
  head_dims: [128, 64]    # Two hidden layers per head
  dropout: 0.3
  num_classes:
    action_type: 4
    card_selection: 15
    card_reservation: 15
    gem_take3: 26
    gem_take2: 5
    noble: 5
    gems_removed: 84

training:
  batch_size: 256
  learning_rate: 0.001
  epochs: 50
  patience: 10
  gradient_clip_norm: null
  scheduler:
    enabled: true
    mode: "min"
    factor: 0.5
    patience: 5

compute:
  device: "cuda"
  num_workers: 12

logging:
  wandb_project: "splendor-ia"
  wandb_entity: null
  log_interval: 1
  save_plots: true

checkpointing:
  save_dir: "data/models"
  save_every: 5
  keep_best_only: false
