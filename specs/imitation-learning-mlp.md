# Feature: MCTS Imitation Learning with Multi-Head Neural Network

## Feature Description
This feature implements a supervised learning system that trains a multi-head neural network to imitate MCTS (Monte Carlo Tree Search) decision-making behavior in Splendor. The system processes game state representations from CSV files generated by MCTS self-play and learns to predict the action that MCTS would select. The architecture uses a shared trunk network that processes the game state, feeding into 7 specialized prediction heads for different action components (action type, card selection, gem choices, etc.). This enables faster inference than full MCTS tree search while maintaining strong playing performance through behavior cloning.

## User Story
As an AI researcher developing a Splendor-playing agent
I want to train a neural network to imitate MCTS decisions from recorded game data
So that I can achieve faster inference times than tree search while maintaining competitive playing strength and create a foundation for future reinforcement learning

## Problem Statement
MCTS provides strong playing performance but is computationally expensive, requiring hundreds of simulations per move. Each game state needs to run tree search with determinization to handle imperfect information, making real-time play slow. Additionally, we need a learned policy network as a foundation for future deep reinforcement learning approaches. The challenge is to compress MCTS knowledge into a fast neural network that can predict good actions from game states alone, while handling the complex multi-component action space of Splendor (choosing action types, target cards, gem combinations, overflow handling, and noble selection).

## Solution Statement
We implement a supervised learning pipeline that:
1. Loads and preprocesses ~1.7M game state/action pairs from CSV files (3500 2-player games, 2054 3-player games, 1705 4-player games)
2. Encodes categorical features (current_player, num_players, player positions) as one-hot vectors
3. Normalizes continuous features (gem counts, victory points, card costs) using StandardScaler fitted on training data
4. Encodes complex action targets into classification tasks (26 classes for gem_take3 combinations, 56 classes for gem removal combinations)
5. Splits data at game-level (not row-level) to prevent leakage: 80% train, 10% validation, 10% test
6. Trains a multi-head neural network with shared trunk (512→256 dimensions) and specialized heads (128→64 dimensions per head)
7. Uses conditional loss computation: only compute loss for relevant heads based on action type (e.g., card_selection head only trains on BUILD actions)
8. Tracks training with Weights & Biases, implements early stopping, and saves best checkpoints
9. Evaluates with comprehensive metrics including per-head accuracy, overall action accuracy, and baseline comparisons

## Relevant Files
Use these files to implement the feature:

### Existing Files
- `data/games/2_games/*.csv` (3500 files) - 2-player game data for training
  - Contains complete game state/action pairs from MCTS self-play
  - Each CSV has 403 columns: metadata, board state (gems, cards, nobles, decks), player states (4 players with gems/reductions/reserved cards), and action labels
  - Action types: "build", "reserve", "take 2 tokens", "take 3 tokens"

- `data/games/3_games/*.csv` (2054 files) - 3-player game data for training
  - Same structure as 2-player with player3/4 columns containing NaN padding

- `data/games/4_games/*.csv` (1705 files) - 4-player game data for training
  - Same structure with all player columns populated

- `pyproject.toml` - Package dependencies managed by uv
  - Will add: torch (CUDA 12.x), scikit-learn, pyyaml, wandb, matplotlib, numpy, tqdm

- `README.md` - Project documentation
  - Will update with ML training instructions after implementation

### New Files

#### Core ML Implementation (`src/imitation_learning/`)
- `src/imitation_learning/__init__.py` - Package initialization, exports main classes
- `src/imitation_learning/config.yaml` - Hyperparameter configuration (model architecture, training params, paths)
- `src/imitation_learning/utils.py` - Helper functions (seed setting, gem combination encoding, mask generation, metrics, visualization)
- `src/imitation_learning/data_preprocessing.py` - Data loading, cleaning, feature engineering, encoding, splitting, normalization, saving
- `src/imitation_learning/dataset.py` - PyTorch Dataset class for loading preprocessed data
- `src/imitation_learning/model.py` - Multi-head neural network architecture (shared trunk + 7 heads)
- `src/imitation_learning/train.py` - Training loop with validation, checkpointing, early stopping, wandb logging
- `src/imitation_learning/evaluate.py` - Comprehensive evaluation metrics, baseline comparisons, visualizations

#### Configuration & Utilities
- `src/imitation_learning/configs/config_small.yaml` - Small architecture for quick testing ([256, 128] trunk, [64] heads)
- `src/imitation_learning/configs/config_medium.yaml` - Medium architecture (default: [512, 256] trunk, [128, 64] heads)
- `src/imitation_learning/configs/config_large.yaml` - Large architecture for capacity ([512, 512, 256] trunk, [128, 128, 64] heads)

#### Data & Outputs
- `data/processed/X_train.npy` - Training features (n_samples × input_dim) after normalization
- `data/processed/X_val.npy` - Validation features
- `data/processed/X_test.npy` - Test features
- `data/processed/labels_train.npz` - Training labels (7 arrays: action_type, card_selection, card_reservation, gem_take3, gem_take2, noble, gems_removed)
- `data/processed/labels_val.npz` - Validation labels
- `data/processed/labels_test.npz` - Test labels
- `data/processed/scaler.pkl` - Fitted StandardScaler for inference
- `data/processed/feature_cols.json` - Feature column names and order for reproducibility
- `data/processed/label_mappings.json` - All encoding mappings (action_type strings→indices, gem combinations→classes, etc.)
- `data/processed/preprocessing_stats.json` - Statistics about preprocessing (input_dim, num_samples, class distributions)
- `data/models/` - Directory for model checkpoints (best_model.pth, latest_model.pth)
- `logs/` - Directory for training logs and wandb local files

#### Documentation
- `docs/hyperparameter_tuning_guide.md` - Guide for experimenting with architectures and training configs

## Implementation Plan

### Phase 1: Foundation
Set up the project structure, install dependencies with proper CUDA support for RTX 4090, and implement core utility functions that will be used throughout the pipeline. This includes reproducibility helpers, gem combination encoding lookup tables, and configuration management.

### Phase 2: Core Implementation
Implement the complete data preprocessing pipeline that loads all CSV files, performs feature engineering and label encoding, splits data at game-level, normalizes features, and saves preprocessed arrays. Then implement the PyTorch Dataset class and multi-head neural network architecture with shared trunk and specialized heads.

### Phase 3: Integration
Integrate the model with the training loop including conditional loss computation, validation, checkpointing, wandb logging, and early stopping. Implement comprehensive evaluation metrics and visualization tools. Connect all components and validate the complete pipeline works end-to-end.

## Step by Step Tasks

### Setup and Dependencies

#### Install ML dependencies with CUDA 12.x support
- Run `uv add torch --index-url https://download.pytorch.org/whl/cu121` to install PyTorch with CUDA 12.1 support (compatible with CUDA 12.9)
- Run `uv add torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121` for complete PyTorch ecosystem
- Run `uv add scikit-learn pyyaml matplotlib seaborn tqdm` for preprocessing, config, and visualization
- Verify wandb is already installed with `uv pip list | grep wandb`
- Verify CUDA availability by running `python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}, Version: {torch.version.cuda}')"`

#### Create directory structure
- Create `src/imitation_learning/` directory with `mkdir -p src/imitation_learning`
- Create `src/imitation_learning/configs/` directory with `mkdir -p src/imitation_learning/configs`
- Create `data/processed/` directory with `mkdir -p data/processed`
- Create `data/models/` directory with `mkdir -p data/models`
- Create `logs/` directory with `mkdir logs`
- Create `docs/` directory if it doesn't exist with `mkdir -p docs`

### Core Utility Functions (`src/imitation_learning/utils.py`)

#### Implement reproducibility helpers
- Write `set_seed(seed: int)` function that sets random seeds for `random`, `np.random`, `torch.manual_seed`, `torch.cuda.manual_seed_all`, and sets `torch.backends.cudnn.deterministic = True`
- Add type hints and docstrings explaining the importance of seed setting for reproducible experiments
- Test the function by running training twice with same seed and verifying identical results

#### Implement gem combination encoding functions
- Write `generate_gem_take3_classes() -> Tuple[Dict[int, Tuple[str, ...]], Dict[Tuple[str, ...], int]]` that creates bidirectional mappings for 26 gem take combinations:
  - Class 0: empty tuple (no gems)
  - Classes 1-5: single gems (white, blue, green, red, black)
  - Classes 6-15: all 2-gem combinations using `itertools.combinations(colors, 2)`
  - Classes 16-25: all 3-gem combinations using `itertools.combinations(colors, 3)`
- Write `generate_gem_removal_classes() -> Tuple[Dict[int, Tuple[int, ...]], Dict[Tuple[int, ...], int]]` that creates mappings for ~56 gem removal combinations:
  - Enumerate all valid combinations of removing 0-3 gems total from 6 types (white, blue, green, red, black, gold)
  - Class 0: (0,0,0,0,0,0) for no removal
  - Use nested loops to generate all valid tuples where sum ≤ 3 and each count ≤ 3
- Write `encode_gem_take3(row: pd.Series, combo_to_class: Dict) -> int` that converts binary columns to class index or -1 if not applicable
- Write `encode_gem_take2(row: pd.Series) -> int` that finds which single gem column is 1 and returns 0-4, or -1 if none
- Write `encode_gems_removed(row: pd.Series, removal_to_class: Dict) -> int` that converts 6 count columns to class index
- Add comprehensive docstrings with examples for each function

#### Implement mask generation helpers
- Write `get_action_type_mask(action_types: np.ndarray, target_type: int) -> np.ndarray` that returns boolean mask for samples matching target action type
- Write `get_valid_label_mask(labels: np.ndarray) -> np.ndarray` that returns boolean mask excluding -1 values
- Write `combine_masks(*masks: np.ndarray) -> np.ndarray` that performs logical AND across multiple masks
- Add unit tests verifying mask generation works correctly

#### Implement metric computation functions
- Write `compute_per_head_accuracy(outputs: Dict[str, torch.Tensor], labels: Dict[str, torch.Tensor], masks: Dict[str, torch.Tensor]) -> Dict[str, float]` that computes accuracy for each head using appropriate masking
- Write `compute_confusion_matrix(predictions: np.ndarray, labels: np.ndarray, num_classes: int) -> np.ndarray` that creates confusion matrix for action_type head
- Write `plot_training_curves(train_losses: List[float], val_losses: List[float], val_accuracies: Dict[str, List[float]], save_path: str)` that creates matplotlib figures
- Write `plot_confusion_matrix(cm: np.ndarray, class_names: List[str], save_path: str)` for visualizing action type confusion
- Write `plot_per_class_accuracy(accuracies: Dict[str, float], save_path: str)` for bar charts of head accuracies

### Configuration Management (`src/imitation_learning/config.yaml`)

#### Create main configuration file
- Create YAML file with sections: seed, data (paths and split ratios), model (architecture params), training (batch size, learning rate, epochs, patience), compute (device, num_workers), logging (wandb_project, log_interval), checkpointing (save_dir, save_every)
- Set seed: 42 for reproducibility
- Set data paths: data_root: "data/games", processed_dir: "data/processed", split ratios: 0.8/0.1/0.1
- Set model architecture: input_dim: null (will be calculated), trunk_dims: [512, 256], head_dims: [128, 64], dropout: 0.3
- Set num_classes: action_type: 4, card_selection: 15, card_reservation: 15, gem_take3: 26, gem_take2: 5, noble: 5, gems_removed: 56
- Set training params: batch_size: 256, learning_rate: 0.001, epochs: 50, patience: 10
- Set compute: device: "cuda", num_workers: 4
- Set logging: wandb_project: "splendor-ia", log_interval: 1
- Set checkpointing: save_dir: "data/models", save_every: 5

#### Create architecture variant configs
- Create `src/imitation_learning/configs/config_small.yaml` with trunk_dims: [256, 128], head_dims: [64], for quick experimentation
- Create `src/imitation_learning/configs/config_medium.yaml` (copy of main config) as default recommended architecture
- Create `src/imitation_learning/configs/config_large.yaml` with trunk_dims: [512, 512, 256], head_dims: [128, 128, 64], for maximum capacity
- Add comments explaining when to use each configuration (small: debugging, medium: default, large: if underfitting)

### Data Preprocessing (`src/imitation_learning/data_preprocessing.py`)

#### Implement data loading and cleaning
- Write `load_all_games(data_root: str) -> pd.DataFrame` function that:
  - Iterates through `data/games/2_games/`, `data/games/3_games/`, `data/games/4_games/` directories
  - For each CSV file, reads with pandas, extracts game_id from filename, adds as column
  - Concatenates all dataframes with `pd.concat(dfs, ignore_index=True)`
  - Applies `df.fillna(0)` to handle NaN values (structured padding for fewer players/nobles)
  - Verifies no unexpected NaN values remain after filling with `df.isna().sum()`
  - Logs total number of samples loaded (expected ~1.7M)
  - Returns single concatenated dataframe
- Add progress bar using tqdm for loading CSVs
- Add error handling for corrupted CSV files (log and skip)

#### Implement column group identification
- Write `identify_column_groups(df: pd.DataFrame) -> Tuple[List[str], List[str], List[str]]` that returns (metadata_cols, label_cols, feature_cols):
  - Metadata columns: game_id, turn_number, current_player, num_players, player0_position through player3_position
  - Label columns: action_type, card_selection, card_reservation, gem_take3_white through gem_take3_black, gem_take2_white through gem_take2_black, noble_selection, gems_removed_white through gems_removed_gold
  - Feature columns: everything else (board state, card properties, player states)
- Validate that all expected columns are present
- Log the count of each column group

#### Implement feature engineering
- Write `engineer_features(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]` that:
  - Converts current_player (0 to num_players-1) to one-hot vector of size 4 (pad for max players)
  - Converts num_players (2, 3, 4) to one-hot vector of size 3
  - Converts player0_position through player3_position to one-hot vectors of size 4 each
  - Removes original categorical columns after one-hot encoding
  - Returns dataframe with new one-hot columns and list of one-hot column names
- Use `pd.get_dummies()` or manual one-hot encoding for control
- Ensure consistent ordering of one-hot columns

#### Implement normalization mask creation
- Write `create_normalization_mask(feature_cols: List[str]) -> np.ndarray` that:
  - Returns boolean array indicating which features should be normalized
  - Set False for all columns ending with "_bonus_white", "_bonus_blue", "_bonus_green", "_bonus_red", "_bonus_black" (already binary)
  - Set False for all one-hot encoded columns (created in previous step)
  - Set True for continuous features: turn_number, gem counts, victory points, reduction counts, card costs, deck remaining counts
- Add validation that mask length matches number of feature columns

#### Implement label encoding
- Write `encode_action_type(action_types: pd.Series) -> np.ndarray` that maps strings to indices:
  - "build" → 0, "reserve" → 1, "take 2 tokens" → 2, "take 3 tokens" → 3
  - Returns integer array with shape (n_samples,)
- Write `encode_card_targets(df: pd.DataFrame, column_name: str) -> np.ndarray` that:
  - Takes card_selection or card_reservation column (values 0-14 or NaN)
  - Replaces NaN with -1 for internal tracking (not a prediction class)
  - Returns integer array where -1 means "skip this sample during training"
- Write `encode_gem_take3_labels(df: pd.DataFrame, combo_to_class: Dict) -> np.ndarray` that:
  - Extracts gem_take3_white through gem_take3_black columns
  - For each row, identifies which gems are selected (value 1)
  - Converts to tuple of selected gem names
  - Maps tuple to class index using lookup table (0-25)
  - Returns -1 if all columns are 0/NaN (not a take 3 action)
- Write `encode_gem_take2_labels(df: pd.DataFrame) -> np.ndarray` that finds which color has value 1 and maps to 0-4, or -1 if none
- Write `encode_noble_selection(noble_col: pd.Series) -> np.ndarray` that keeps values 0-4 or -1 unchanged
- Write `encode_gems_removed_labels(df: pd.DataFrame, removal_to_class: Dict) -> np.ndarray` that:
  - Extracts gems_removed_white through gems_removed_gold columns
  - Converts 6-tuple of counts to class index using lookup table
  - Returns 0 for (0,0,0,0,0,0) and appropriate class for other combinations
- Combine all encoding functions into `encode_all_labels(df: pd.DataFrame) -> Dict[str, np.ndarray]` that returns dict of 7 label arrays

#### Implement game-level data splitting
- Write `split_by_game_id(df: pd.DataFrame, train_ratio: float, val_ratio: float, test_ratio: float, seed: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]` that:
  - Extracts unique game_id values
  - Uses sklearn's GroupShuffleSplit to split at game level (prevents leakage)
  - First split: 80% train vs 20% temp
  - Second split: 50% of temp for val vs 50% for test (gives 10% each)
  - Returns three arrays of row indices for train, val, test
  - Sets random seed before splitting
- Verify that no game_id appears in multiple splits
- Log the number of games and rows in each split

#### Implement feature normalization
- Write `normalize_features(X_train: np.ndarray, X_val: np.ndarray, X_test: np.ndarray, norm_mask: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, StandardScaler]` that:
  - Extracts continuous features from X_train using norm_mask
  - Fits StandardScaler on continuous training features only
  - Transforms continuous features in train/val/test using fitted scaler
  - Reconstructs full feature matrices by inserting normalized features back
  - Returns normalized X_train, X_val, X_test and fitted scaler
- Ensure scaler statistics are only computed from training data (no leakage)

#### Implement preprocessing pipeline orchestration
- Write `main()` function that:
  - Loads config from YAML
  - Sets seed for reproducibility
  - Loads all game data from CSVs
  - Identifies column groups
  - Engineers one-hot features
  - Creates normalization mask
  - Encodes all labels
  - Splits data by game_id
  - Extracts features (excluding game_id from state)
  - Normalizes features
  - Calculates actual input_dim after one-hot encoding
  - Saves preprocessed data: X_train.npy, X_val.npy, X_test.npy, labels_train.npz, labels_val.npz, labels_test.npz
  - Saves scaler.pkl using pickle
  - Saves feature_cols.json, label_mappings.json, preprocessing_stats.json (including input_dim)
  - Logs summary statistics (dataset sizes, class distributions, input_dim)
- Add CLI argument parsing for config file path
- Make script executable with `if __name__ == "__main__"`

#### Test preprocessing pipeline
- Run preprocessing script: `cd src/imitation_learning && python data_preprocessing.py --config config.yaml`
- Verify all output files are created in `data/processed/`
- Check preprocessing_stats.json for input_dim value
- Verify no NaN values in saved arrays
- Check class distributions are reasonable (no class has 0 samples except rare events)
- Verify train/val/test splits have no game overlap

### PyTorch Dataset Class (`src/imitation_learning/dataset.py`)

#### Implement SplendorDataset class
- Create class `SplendorDataset(torch.utils.data.Dataset)` with `__init__(self, X: np.ndarray, labels: Dict[str, np.ndarray])`
- Store features as float32 tensor and labels as dict of long tensors
- Implement `__len__(self)` returning number of samples
- Implement `__getitem__(self, idx: int) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]` that returns:
  - state_tensor: float tensor shape (input_dim,)
  - labels_dict: dict with keys {action_type, card_selection, card_reservation, gem_take3, gem_take2, noble, gems_removed}, each a scalar long tensor
- Ensure -1 labels are preserved (PyTorch cross_entropy will ignore with ignore_index=-1)
- Add docstrings explaining the data format

#### Implement data loader creation utilities
- Write `create_dataloaders(batch_size: int, num_workers: int) -> Tuple[DataLoader, DataLoader, DataLoader]` that:
  - Loads preprocessed data from data/processed/
  - Creates SplendorDataset instances for train, val, test
  - Creates DataLoader for each with appropriate shuffle (True for train, False for val/test)
  - Returns train_loader, val_loader, test_loader
- Add error handling for missing preprocessed data files

#### Test dataset implementation
- Write simple test script that loads data and iterates through one batch
- Verify tensor shapes and dtypes are correct
- Verify labels_dict contains all 7 keys
- Verify -1 values are preserved in labels

### Neural Network Architecture (`src/imitation_learning/model.py`)

#### Implement shared trunk network
- Create class `SharedTrunk(nn.Module)` with `__init__(self, input_dim: int, trunk_dims: List[int], dropout: float)`
- Build sequential layers: for each dim in trunk_dims, add Linear → ReLU → Dropout
- Use nn.ModuleList to store layers for proper parameter registration
- Implement `forward(self, x: torch.Tensor) -> torch.Tensor` that passes input through all layers
- Add docstring explaining trunk learns shared representation

#### Implement prediction head networks
- Create class `PredictionHead(nn.Module)` with `__init__(self, input_dim: int, head_dims: List[int], num_classes: int, dropout: float)`
- Build sequential layers: for each dim in head_dims, add Linear → ReLU → Dropout
- Add final Linear layer to num_classes (no activation, output raw logits)
- Implement `forward(self, x: torch.Tensor) -> torch.Tensor` returning logits
- Add docstring explaining head predicts among num_classes

#### Implement multi-head model
- Create class `MultiHeadSplendorNet(nn.Module)` with `__init__(self, config: Dict)`
- Initialize shared trunk using config['input_dim'], config['trunk_dims'], config['dropout']
- Create 7 prediction heads in nn.ModuleDict with keys: action_type, card_selection, card_reservation, gem_take3, gem_take2, noble, gems_removed
- Each head uses trunk output dim (trunk_dims[-1]), config['head_dims'], config['num_classes'][head_name], config['dropout']
- Implement `forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]` that:
  - Passes x through shared trunk
  - Passes trunk output through each head independently
  - Returns dict mapping head_name to logits tensor
- Add docstring explaining conditional querying during inference
- Optionally implement Xavier/He initialization in `_init_weights()` method

#### Test model architecture
- Write test script that creates model with dummy config
- Do forward pass with random batch, verify output dict has 7 keys
- Verify output shapes are (batch_size, num_classes) for each head
- Verify model can be moved to CUDA and runs on GPU
- Print model summary showing number of parameters

### Training Loop (`src/imitation_learning/train.py`)

#### Implement loss computation functions
- Write `compute_action_type_loss(logits: torch.Tensor, labels: torch.Tensor, weight: Optional[torch.Tensor] = None) -> torch.Tensor` that:
  - Always computes loss (every sample has action type)
  - Uses F.cross_entropy(logits, labels, weight=weight)
  - Returns scalar loss
- Write `compute_conditional_loss(logits: torch.Tensor, labels: torch.Tensor, mask: torch.Tensor, ignore_index: int = -1) -> torch.Tensor` that:
  - Applies mask to filter relevant samples
  - Only computes F.cross_entropy on masked samples
  - Returns 0.0 if no valid samples, else mean loss
  - Used for card_selection, card_reservation, gem_take3, gem_take2, noble heads
- Write `compute_gems_removed_loss(logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor` that computes loss when label != 0
- Write `compute_total_loss(outputs: Dict[str, torch.Tensor], labels: Dict[str, torch.Tensor], action_types: torch.Tensor, class_weights: Optional[Dict] = None) -> Tuple[torch.Tensor, Dict[str, float]]` that:
  - Computes action_type loss (always)
  - Computes card_selection loss (mask: action_type == 0 AND label != -1)
  - Computes card_reservation loss (mask: action_type == 1 AND label != -1)
  - Computes gem_take3 loss (mask: action_type == 3 AND label != -1)
  - Computes gem_take2 loss (mask: action_type == 2 AND label != -1)
  - Computes noble loss (mask: action_type == 0 AND label != -1)
  - Computes gems_removed loss (mask: label != 0)
  - Sums all losses (unweighted for now)
  - Returns total loss and dict of individual losses for logging

#### Implement class weight computation
- Write `compute_class_weights(labels: np.ndarray, num_classes: int) -> torch.Tensor` that:
  - Counts frequency of each class in training data
  - Computes inverse frequency weights
  - Normalizes weights to sum to num_classes
  - Returns tensor of weights for use in cross_entropy
- Write `compute_all_class_weights(labels_dict: Dict[str, np.ndarray], num_classes_dict: Dict[str, int]) -> Dict[str, torch.Tensor]` that computes weights for each head
- Make class weighting optional via config flag

#### Implement training epoch
- Write `train_one_epoch(model: nn.Module, dataloader: DataLoader, optimizer: torch.optim.Optimizer, device: torch.device, class_weights: Optional[Dict] = None) -> Tuple[float, Dict[str, float]]` that:
  - Sets model to train mode
  - Iterates through batches with tqdm progress bar
  - Moves batch to device
  - Zero gradients
  - Forward pass
  - Compute total loss and per-head losses
  - Backward pass
  - Optimizer step
  - Accumulate losses
  - Returns average total loss and dict of average per-head losses

#### Implement validation
- Write `validate(model: nn.Module, dataloader: DataLoader, device: torch.device) -> Tuple[float, Dict[str, float], Dict[str, float]]` that:
  - Sets model to eval mode
  - Disables gradients with torch.no_grad()
  - Iterates through batches
  - Forward pass
  - Compute losses
  - Compute accuracies for each head with appropriate masking
  - Returns average loss, dict of per-head losses, dict of per-head accuracies

#### Implement checkpointing
- Write `save_checkpoint(model: nn.Module, optimizer: torch.optim.Optimizer, epoch: int, val_loss: float, val_acc: Dict[str, float], save_path: str)` that saves:
  - model.state_dict()
  - optimizer.state_dict()
  - epoch number
  - validation metrics
- Write `load_checkpoint(model: nn.Module, optimizer: torch.optim.Optimizer, checkpoint_path: str) -> Tuple[int, float, Dict[str, float]]` that loads checkpoint and returns epoch, val_loss, val_acc
- Implement best model tracking (save when validation improves)

#### Implement early stopping
- Write `EarlyStopping` class that:
  - Tracks best validation loss
  - Counts epochs since improvement
  - Returns True when patience exceeded
  - Optionally loads best model weights when stopping

#### Implement main training loop
- Write `main()` function that:
  - Loads config from YAML (with CLI override support)
  - Loads preprocessing_stats.json to get input_dim
  - Updates config with calculated input_dim
  - Sets seed for reproducibility
  - Creates data loaders
  - Initializes model and moves to device
  - Initializes Adam optimizer with config learning_rate
  - Initializes ReduceLROnPlateau scheduler
  - Initializes wandb with project name, logs config and model architecture
  - Optionally computes class weights
  - Initializes early stopping
  - For each epoch:
    - Train one epoch, log train losses
    - Validate, log val losses and accuracies
    - Log to wandb
    - Step scheduler based on val loss
    - Save checkpoint if validation improved (both best and every N epochs)
    - Check early stopping
  - Save final model
  - Print best metrics achieved
  - Close wandb
- Add CLI argument parsing for config path and optional overrides (learning_rate, batch_size, etc.)
- Make script executable

#### Test training pipeline
- Run short training test with small data subset and 2 epochs: `cd src/imitation_learning && python train.py --config config.yaml --epochs 2`
- Verify losses decrease
- Verify checkpoints are saved to data/models/
- Verify wandb logs appear in logs/ directory
- Verify CUDA is utilized (check GPU usage with nvidia-smi during training)

### Evaluation and Metrics (`src/imitation_learning/evaluate.py`)

#### Implement per-head metrics
- Write `evaluate_action_type(model: nn.Module, dataloader: DataLoader, device: torch.device) -> Dict` that computes:
  - Overall accuracy
  - Per-class accuracy (BUILD, RESERVE, TAKE 2, TAKE 3)
  - Confusion matrix
  - Returns dict with all metrics
- Write `evaluate_conditional_head(model: nn.Module, dataloader: DataLoader, device: torch.device, head_name: str, action_type_filter: Optional[int] = None) -> Dict` that computes:
  - Accuracy among applicable samples (using action_type mask)
  - Distribution of predictions vs. true labels
  - Most common errors
  - Frequency of applicable samples in dataset

#### Implement aggregate metrics
- Write `compute_overall_action_accuracy(model: nn.Module, dataloader: DataLoader, device: torch.device) -> float` that computes:
  - Percentage of samples where action_type is correct AND corresponding target head prediction is correct
  - This is the true end-to-end imitation learning performance metric
- Add detailed logging showing breakdown by action type

#### Implement baseline comparisons
- Write `RandomBaseline` class that predicts random actions:
  - Action type: uniform random among 4 classes
  - Targets: uniform random among legal class indices
- Write `HeuristicBaseline` class that uses simple rules:
  - Always BUILD if possible (check if any card can be built)
  - Always buy cheapest legal card
  - Otherwise always TAKE 3 if possible, else TAKE 2
- Write `evaluate_baselines(dataloader: DataLoader) -> Dict[str, Dict]` that computes metrics for both baselines
- Compare baseline performance to model performance

#### Implement visualization
- Write `plot_all_metrics(metrics: Dict, save_dir: str)` that creates:
  - Action type confusion matrix heatmap
  - Per-head accuracy bar chart
  - Overall action accuracy comparison (model vs baselines)
  - Loss curves from training history
- Save all plots to logs/ directory with timestamps

#### Implement main evaluation script
- Write `main()` function that:
  - Loads config
  - Loads best checkpoint from data/models/
  - Creates test dataloader
  - Evaluates all per-head metrics
  - Computes overall action accuracy
  - Evaluates baselines
  - Generates all visualizations
  - Saves comprehensive results report to logs/evaluation_results.json
  - Prints summary table to console
- Add CLI argument for checkpoint path
- Make script executable

#### Test evaluation pipeline
- Run evaluation on test set: `cd src/imitation_learning && python evaluate.py --checkpoint data/models/best_model.pth`
- Verify all metrics are computed
- Verify visualizations are saved
- Verify results JSON is created
- Compare action_type accuracy to expected 70-85% range
- Verify model significantly outperforms random baseline (25%)

### Package Integration and Documentation

#### Create package initialization
- Write `src/imitation_learning/__init__.py` that exports:
  - Main model class: MultiHeadSplendorNet
  - Dataset class: SplendorDataset
  - Utility functions: set_seed, create_dataloaders
  - Version string: __version__ = "0.1.0"
- Add package-level docstring describing the imitation learning system

#### Create hyperparameter tuning guide
- Write `docs/hyperparameter_tuning_guide.md` with sections:
  - How to create new config files
  - Recommended experiments (architecture variations, learning rates, batch sizes)
  - How to interpret training curves
  - Common issues and solutions (overfitting, underfitting, NaN loss, etc.)
  - Grid search recommendations
  - Expected performance ranges for different architectures
- Include examples of good vs bad training curves

#### Update main README
- Add new section "Phase 2: Imitation Learning" to README.md after Phase 1
- Document how to run preprocessing: `cd src/imitation_learning && python data_preprocessing.py`
- Document how to run training: `python train.py --config config.yaml`
- Document how to evaluate: `python evaluate.py --checkpoint data/models/best_model.pth`
- Document expected performance metrics
- Link to hyperparameter tuning guide
- Add troubleshooting section for common issues

### End-to-End Testing and Validation

#### Run full preprocessing pipeline
- Run preprocessing on all data: `cd src/imitation_learning && python data_preprocessing.py --config config.yaml`
- Verify output files in data/processed/
- Check preprocessing_stats.json for:
  - Total samples (~1.7M expected)
  - Calculated input_dim value (expected 400-450)
  - Train/val/test splits (verify ~80/10/10%)
  - Class distributions (check for severe imbalance)
- Verify no errors or warnings in logs

#### Run training with multiple configurations
- Train small model (quick test): `python train.py --config configs/config_small.yaml --epochs 10`
- Verify training completes without errors
- Check wandb dashboard shows training curves
- Train medium model (default): `python train.py --config configs/config_medium.yaml --epochs 50`
- Monitor training for ~1-2 hours depending on GPU
- Verify early stopping triggers if patience exceeded
- Check best model is saved to data/models/best_model.pth

#### Run comprehensive evaluation
- Evaluate on test set: `python evaluate.py --checkpoint data/models/best_model.pth`
- Verify action_type accuracy is 70-85%
- Verify overall action accuracy is 50-70%
- Verify model outperforms random baseline by large margin (>3x)
- Verify visualizations show reasonable confusion matrices (not all predictions in one class)
- Check for any heads with very low accuracy (<30%) indicating potential issues

#### Verify CUDA utilization
- During training, run `watch -n 1 nvidia-smi` in separate terminal
- Verify GPU memory usage is significant (8-16 GB depending on batch size)
- Verify GPU utilization is high (>80%) during training batches
- Verify training is significantly faster than CPU-only

## Testing Strategy

### Unit Tests

#### Data Preprocessing Tests
- Test `generate_gem_take3_classes()` returns exactly 26 classes with correct mapping
- Test `generate_gem_removal_classes()` returns ~56 classes with all valid combinations
- Test `encode_gem_take3()` correctly maps binary columns to class indices
- Test `encode_action_type()` maps all 4 strings correctly
- Test one-hot encoding produces correct dimensions
- Test normalization mask correctly identifies continuous vs categorical features
- Test game-level splitting ensures no game appears in multiple splits
- Test StandardScaler is only fitted on training data

#### Dataset Tests
- Test SplendorDataset returns tensors with correct shapes and dtypes
- Test __len__ returns correct number of samples
- Test __getitem__ handles -1 labels correctly
- Test DataLoader batching works correctly with num_workers > 0

#### Model Tests
- Test SharedTrunk forward pass with different input dimensions
- Test PredictionHead produces correct output shape
- Test MultiHeadSplendorNet forward pass returns dict with 7 keys
- Test model can be moved to CUDA without errors
- Test model parameter count is reasonable (not too small/large)

#### Loss Computation Tests
- Test action_type loss is computed for all samples
- Test conditional loss is only computed for masked samples
- Test conditional loss returns 0.0 when mask is all False
- Test total loss is sum of individual losses
- Test class weights are correctly computed from label distribution

### Integration Tests

#### End-to-End Preprocessing Test
- Load small subset of CSVs (10 games)
- Run complete preprocessing pipeline
- Verify output files are created
- Verify no NaN values in outputs
- Verify input_dim calculation is correct
- Verify label encodings are reversible

#### End-to-End Training Test
- Use small preprocessed dataset (1000 samples)
- Train for 5 epochs with small batch size
- Verify training loss decreases
- Verify validation metrics are computed
- Verify checkpoints are saved
- Verify wandb logging works

#### End-to-End Evaluation Test
- Load trained model from checkpoint
- Run evaluation on small test set (100 samples)
- Verify all metrics are computed without errors
- Verify visualizations are created
- Verify results JSON is valid

### Edge Cases

#### Data Edge Cases
- Games with only 2 players (player2/3 columns are NaN)
- Actions where noble_selection = -1 (no noble acquired)
- Overflow cases where gems_removed is needed (rare)
- Reserved cards that are never used (NaN in reserved slots)
- Actions at end of game (may have unusual state)

#### Training Edge Cases
- Batch where all samples have same action_type (loss still computes)
- Batch where no samples have noble selection (head loss is 0.0)
- Very rare classes (reserve action, gem removal) with few samples
- Learning rate too high causing NaN loss (verify gradient clipping or LR reduction)
- Model overfitting (validation loss increases while training loss decreases)

#### Model Edge Cases
- Input with all zeros (model should still produce valid predictions)
- Input with very large values (normalization should prevent issues)
- Prediction during inference when action_type is BUILD but no cards available (model doesn't know game rules)
- Querying wrong head for action type (code should only query appropriate heads)

## Acceptance Criteria

1. **Data Preprocessing Complete**
   - All 7,259 CSV files (3500 + 2054 + 1705) successfully loaded and concatenated
   - No NaN values remain after filling (except in label -1 placeholders)
   - Train/val/test splits have 80/10/10% ratio at game level with no overlap
   - Input dimension correctly calculated and saved to preprocessing_stats.json (expected 400-450)
   - All 7 label arrays correctly encoded with proper class ranges
   - Preprocessed files saved to data/processed/ and loadable

2. **Model Architecture Implemented**
   - MultiHeadSplendorNet class successfully initializes with config
   - Forward pass produces dict with 7 heads: {action_type, card_selection, card_reservation, gem_take3, gem_take2, noble, gems_removed}
   - Each head outputs correct shape: (batch_size, num_classes)
   - Model successfully moves to CUDA and runs on RTX 4090
   - Total parameter count is reasonable (1-10M parameters depending on architecture)

3. **Training Pipeline Functional**
   - Training runs for full 50 epochs or until early stopping
   - Training loss decreases over epochs (evidence of learning)
   - Validation loss computed correctly with conditional masking
   - Checkpoints saved to data/models/ when validation improves
   - Wandb logs show training curves for all metrics
   - GPU utilization >80% during training (efficient CUDA usage)

4. **Model Performance Meets Expectations**
   - Action type accuracy: 70-85% (primary metric)
   - Overall action accuracy: 50-70% (action_type correct AND target correct)
   - Model outperforms random baseline (25% action type) by >3x
   - No head has accuracy below 30% (indicating critical failure)
   - Common action types (BUILD, TAKE 3) achieve 75-90% accuracy
   - Model predicts reasonable distributions (not all one class)

5. **Evaluation and Analysis Complete**
   - Evaluation script runs on test set without errors
   - Per-head metrics computed and saved
   - Confusion matrix shows model learns all 4 action types (not collapsed to one)
   - Baseline comparisons show clear improvement over random and heuristic
   - Visualizations (plots, confusion matrices) generated and saved
   - Comprehensive results JSON saved to logs/

6. **Code Quality Standards Met**
   - All functions have type hints for parameters and return values
   - All public functions have docstrings with purpose, args, returns
   - Code follows PEP 8 style (checked with ruff or flake8)
   - No hardcoded paths (all configurable via YAML)
   - Error handling for file I/O and data loading
   - Logging used appropriately (info for progress, warning for issues)

7. **Documentation Complete**
   - README.md updated with Phase 2 instructions
   - Hyperparameter tuning guide created with examples
   - Config files have comments explaining parameters
   - Code comments explain complex logic (especially loss masking)
   - Expected performance ranges documented

## Validation Commands
Execute every command to validate the feature works correctly with zero regressions.

- `python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}, Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')"` - Verify PyTorch CUDA setup
- `cd src/imitation_learning && python data_preprocessing.py --config config.yaml` - Run complete preprocessing pipeline on all game data
- `python -c "import json; stats = json.load(open('data/processed/preprocessing_stats.json')); print(f\"Total samples: {stats['total_samples']}, Input dim: {stats['input_dim']}, Train size: {stats['train_size']}\")"` - Verify preprocessing output
- `cd src/imitation_learning && python train.py --config configs/config_small.yaml --epochs 5` - Quick training test with small architecture
- `cd src/imitation_learning && python train.py --config configs/config_medium.yaml` - Full training with default architecture (50 epochs or early stop)
- `cd src/imitation_learning && python evaluate.py --checkpoint ../data/models/best_model.pth` - Evaluate best model on test set
- `python -c "import json; results = json.load(open('logs/evaluation_results.json')); print(f\"Action type acc: {results['action_type_accuracy']:.3f}, Overall acc: {results['overall_accuracy']:.3f}\")"` - Check final metrics
- `ls -lh data/processed/` - Verify all preprocessed files exist (X_train.npy, labels_train.npz, scaler.pkl, etc.)
- `ls -lh data/models/` - Verify model checkpoints saved (best_model.pth, latest_model.pth)
- `ls -lh logs/` - Verify training logs and evaluation results exist

## Notes

### Dependencies Added
The following packages will be added via `uv add`:
- **torch, torchvision, torchaudio** (with CUDA 12.1 support for RTX 4090)
- **scikit-learn** (for StandardScaler, train_test_split, metrics)
- **pyyaml** (for config file parsing)
- **matplotlib** (for training curves and visualizations)
- **seaborn** (for confusion matrix heatmaps)
- **tqdm** (for progress bars)
- **wandb** (already installed, for experiment tracking)
- **numpy** (likely already available, for array operations)

### Expected Training Time
With RTX 4090 and ~1.7M samples:
- Preprocessing: 10-20 minutes (loading CSVs, encoding, splitting)
- Training (50 epochs, batch_size=256): 2-4 hours
- Evaluation: 5-10 minutes
- Total: 3-5 hours for complete pipeline

### GPU Memory Usage
- Expected GPU memory: 8-16 GB depending on batch size
- RTX 4090 has 24 GB VRAM, so plenty of headroom
- Can increase batch_size to 512 or 1024 if desired for faster training

### Key Implementation Notes

1. **Conditional Loss Masking**: The most critical aspect is correctly masking losses for each head. Only compute loss when the head is applicable (e.g., card_selection only when action_type==BUILD). This prevents the model from learning spurious correlations.

2. **No -1 Class in Model Outputs**: The model never predicts -1. The -1 values in labels are only for masking during training. Each head predicts among its valid classes (0-14 for cards, 0-25 for gem_take3, etc.). During inference, you conditionally query heads based on action_type prediction.

3. **Game-Level Splitting**: CRITICAL to split by game_id, not randomly by rows. Otherwise, consecutive states from the same game will appear in both train and test, causing severe overfitting and inflated test accuracy.

4. **Normalization After Splitting**: Must fit StandardScaler only on training data, then apply to val/test. Fitting on all data would leak information.

5. **Class Imbalance**: Action types are likely imbalanced (more BUILD and TAKE 3 than RESERVE). Consider weighted loss if model collapses to most common class. Check class distributions in preprocessing_stats.json.

6. **Reproducibility**: Set seeds everywhere (random, numpy, torch, torch.cuda) and use `torch.backends.cudnn.deterministic = True` to ensure identical results across runs with same seed.

7. **Input Dimension Calculation**: The exact input_dim will be calculated during preprocessing after one-hot encoding. It depends on the number of original features plus the new one-hot columns. Expect 400-450 based on the guide's estimate.

### Future Enhancements (Out of Scope)

These are documented for future work but not required for this feature:
- **Illegal action detection**: Implement game rule checker to measure how often model predicts illegal actions
- **Grid search**: Automated hyperparameter search over architectures and learning rates
- **Transformer architecture**: Attention-based model as alternative to MLP
- **Player permutation augmentation**: Data augmentation exploiting player symmetry
- **Self-play evaluation**: Measure win rate by actually playing games (requires game simulator integration)
- **Ensemble methods**: Train multiple models with different seeds and average predictions
- **Explainability**: Gradient-based methods or attention to understand decision-making

### Questions Answered
1. ✅ Input dimension: Will be calculated during preprocessing after one-hot encoding
2. ✅ Weighted loss: Implement as optional, enable if class imbalance is problematic
3. ✅ Initial architecture: Use medium config ([512, 256] trunk, [128, 64] heads) as default
4. ✅ Validation frequency: Every epoch (dataset is large enough)
5. ✅ Gem removal classes: Verify during implementation, expect ~56 classes for removing 0-3 gems from 6 types
6. ✅ CUDA version: 12.9, compatible with PyTorch CUDA 12.1 builds
7. ✅ Project structure: `src/imitation_learning/` for ML code, `data/processed/` for preprocessed arrays
